{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae1efe6-09af-40eb-8945-347e633eb026",
   "metadata": {},
   "source": [
    "# Performance Measures "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615dbf6-70dd-46be-850c-ce00adf608f0",
   "metadata": {},
   "source": [
    "Similar as for the regression problem, we can run a cross-validation to determine how often the prediction of a model is correct. This can be done by selecting \"accuracy\" as measure which is optimized during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6e16b6-fb73-46cc-9710-ff6b0a32d5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of each set:\n",
      "[0.9425 0.9525 0.955  0.96   0.9425]\n",
      "\n",
      "This is an average accuracy of 0.951.\n"
     ]
    }
   ],
   "source": [
    "# code of a previous notebook of this section\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data = pd.read_csv('cl1_data.csv')\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = train_set['label']\n",
    "X_train = train_set.drop(['label'], axis=1)\n",
    "\n",
    "num_pipeline = make_pipeline(StandardScaler()) \n",
    "\n",
    "preprocessing = ColumnTransformer([(\"num\",num_pipeline, make_column_selector(dtype_include=np.number))])\n",
    "\n",
    "model_svc = make_pipeline(preprocessing, SVC(kernel='rbf', C=1.0)) \n",
    "model_svc.fit(X_train, y_train)\n",
    "# end of code from a previous notebook of this section\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_accuracy = cross_val_score(model_svc, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "print(f\"Accuracy of each set:\\n{scores_accuracy}\\n\")\n",
    "print(\"This is an average accuracy of %0.3f.\" % (scores_accuracy.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c1d5e-efac-4e6c-a5b9-6e9d2e757262",
   "metadata": {},
   "source": [
    "Thus, 95 % of the labels \"group1\" and \"group2\" are correctly assigned for the training data set. This sounds really good. But how good is the accuracy of a classification algorithm assigning the label \"group1\" independent of the input features to our data set:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cefc54-09cf-4102-82d9-8f3b751e3cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of each set:\n",
      "[0.795  0.795  0.7975 0.7975 0.7975]\n",
      "\n",
      "This is an average accuracy of 0.796.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "model_dummy = DummyClassifier(strategy='prior')\n",
    "model_dummy.fit(X_train, y_train)\n",
    "\n",
    "scores_dummy = cross_val_score(model_dummy, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "print(f\"Accuracy of each set:\\n{scores_dummy}\\n\")\n",
    "print(\"This is an average accuracy of %0.3f.\" % (scores_dummy.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be02d2b-655e-4eb1-8d18-a0a015ac14bb",
   "metadata": {},
   "source": [
    "We have already an accuracy close to 80% since 80% of the instances have the label \"group1\". Thus, the accuracy is an overall poor metric for validation since it is strongly affected by the number of instances per class. A better choice might be metrics based on the confusion matrix. The confusion matrix can be calculated by scikit-learn as follow:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08cd0af-fe33-4a9a-8977-124fea200312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1531   62]\n",
      " [  37  370]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_svc_pred = cross_val_predict(model_svc, X_train, y_train, cv=5)\n",
    "\n",
    "cm = confusion_matrix(y_train, y_train_svc_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d58f6e-7394-4ccc-a472-668007f70c63",
   "metadata": {},
   "source": [
    "The confusion matrix consists of 4 entries. The first entry at row 1, column 1 is the number of true false ($TF$) instances. These instances have the label \"group1\" (false, not group2) and were correctly classified as \"group1\" (false). The entry at row 1, column 2 are false positive ($FP$) instances. These instances have the label \"group1\" (false) but were predicted as \"group2\" (true). The number of false negative ($FN$) instances is on row 2, column 1. These instances have the label \"group2\" (true) but were predicted as \"group1\" (false). Finally, we have the number of true positive ($TP$) instances on row 2, column 2. These are the instances correctly predicted as \"group2\" (true). \n",
    "\n",
    "![image](img/error_matrix.png)\n",
    "\n",
    "Thus, the sum of the first row is the number of the \"group1\" (false) labels in the training data set while the sum of the second row is the number of \"group2\" (true) labels in the training data set.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a9ab4b-bab8-4d21-86a1-1adf428eae0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "group1    1593\n",
       "group2     407\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748bfe57-2860-401f-b019-0eb19d0f3fbc",
   "metadata": {},
   "source": [
    "The sum of the first column refers to instances predicted as \"group1\" (false) while the second column refers to the sum of instances predicted as \"group2\" (true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5401c9-f88d-4fc8-a92e-c84ebb844947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group1    1568\n",
       "group2     432\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train_svc_pred).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5376f065-8d3d-4ea6-b57e-16e3add3c62e",
   "metadata": {},
   "source": [
    "We can use the entries of the confusion matrix to calculate different metrics. The first one is precision:\n",
    "\n",
    "$precision = \\frac{TP}{FP+TP}$\n",
    "\n",
    "A precision close to 1 highlights that instances predicted as \"group2\" (true) are correctly predicted. Thus, we can trust our \"group2\" predictions that they are really an element of the \"group2\" class. We can access precision for our cross validation by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1715f480-c405-4dfd-94ce-ba9dab88023e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of each set:\n",
      "[0.82417582 0.85393258 0.86206897 0.87356322 0.87179487]\n",
      "\n",
      "This is an average precision of 0.857 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, make_scorer\n",
    "\n",
    "precision = make_scorer(precision_score, pos_label='group2')\n",
    "\n",
    "scores_precision = cross_val_score(model_svc, X_train, y_train, cv=5, scoring=precision)\n",
    "print(f\"Precision of each set:\\n{scores_precision}\\n\")\n",
    "print(\"This is an average precision of %0.3f \\n\" % (scores_precision.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f2376-11d8-4948-b6af-7ca967cfc029",
   "metadata": {},
   "source": [
    "If you want to determine the precision for the test data set, use following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c31a165-24d8-4f0d-a583-d9cb32b53b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a precision of 0.80 on the test data set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = test_set['label']\n",
    "X_test = test_set.drop(['label'], axis=1)\n",
    "\n",
    "y_pred_test = model_svc.predict(X_test)\n",
    "precision_test = precision_score(y_test, y_pred_test, pos_label='group2')\n",
    "print(\"The model has a precision of %0.2f on the test data set\\n\" % precision_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3aea7-9d51-457e-ad1b-bacf4e54c957",
   "metadata": {},
   "source": [
    "A classifier that always makes \"group2\" predictions except for a single instance with the highest confidence for \"group1\" will have a precision of one. This is not desirable. Therefore, a complementary metric is reasonable. This is the recall, also called sensitivity or the true positive rate:\n",
    "\n",
    "$recall = \\frac{TP}{FN+TP}$\n",
    "\n",
    "We can access it for our cross validation by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3993237d-e89d-4864-a3c4-c9d492af080c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of each set:\n",
      "[0.91463415 0.92682927 0.92592593 0.9382716  0.83950617]\n",
      "\n",
      "This is an average recall of 0.909 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall = make_scorer(recall_score, pos_label='group2')\n",
    "\n",
    "scores_recall_svc = cross_val_score(model_svc, X_train, y_train, cv=5, scoring=recall)\n",
    "\n",
    "print(f\"Recall of each set:\\n{scores_recall_svc}\\n\")\n",
    "print(\"This is an average recall of %0.3f \\n\" % (scores_recall_svc.mean())) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab92c3-37f6-47e6-ac75-0cc710534528",
   "metadata": {},
   "source": [
    "Precision and recall are combined in the $F_1$ score given as:\n",
    "\n",
    "$F_1 = \\frac{2}{\\frac{1}{precision}+\\frac{1}{recall}} = \\frac{2TP}{2TP+FP+FN}$\n",
    "\n",
    "Thus, a high $F_1$ score refers to a high recall and precision since it is the harmonic mean of precision and recall. Please note, the harmonic mean is stronger affected by small values than the mean value. You can get the $F_1$ score by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4314ded4-2c1f-41a9-8357-31d8cd8b8977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of each set:\n",
      "[0.86705202 0.88888889 0.89285714 0.9047619  0.85534591]\n",
      "\n",
      "This is an average f1-score of 0.882 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = make_scorer(f1_score, pos_label='group2')\n",
    "\n",
    "scores_f1_svc = cross_val_score(model_svc, X_train, y_train, cv=5, scoring=f1)\n",
    "\n",
    "print(f\"F1-score of each set:\\n{scores_f1_svc}\\n\")\n",
    "print(\"This is an average f1-score of %0.3f \\n\" % (scores_f1_svc.mean())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a28d90-4453-490f-9b14-74609d469247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ai-school",
   "language": "python",
   "name": "venv-ai-school"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
