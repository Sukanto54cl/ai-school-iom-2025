{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02601ac1-0f28-40f1-888f-09b27d386793",
   "metadata": {},
   "source": [
    "# Boosting (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a67f8-2dab-4b01-b1a6-ffe70550e8de",
   "metadata": {},
   "source": [
    "Boosting refers to any ensemble methods that combines several regression models to a better one. The general idea is to train predictors sequentially, each trying to correct its predecessor. The most popular are Adaboost and gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8fbe1e-56f2-4fd3-84e1-9cd283e627af",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e3a14-6f17-47b0-91c4-fbadd71d4b0a",
   "metadata": {},
   "source": [
    "AdaBoost trains an initial regression model. Subsequently, the obtained model is used to make predictions on the training set. This allows to determine the error of the model (e.g. $|y_i-h_t(x_i)|$) which will be used to update the old weights $w_{i}^{t}$ as follow: \n",
    "\n",
    "$\n",
    "w_{i}^{t+1} = w_{i}^{t} \\cdot \\eta \\cdot \\left(\\frac{e_t}{1-e_t}\\right)^{1-L_i\\left(\\frac{|y_i-h_t(x_i)|}{\\max|y_i-h_t(x_i)|}\\right)}\n",
    "$\n",
    "\n",
    "where $e_t$ is calculated by:\n",
    "\n",
    "$\n",
    "e_t = \\sum\\limits_{i=1}^m w_i^t \\cdot L_i\\left(\\frac{|y_i-h_t(x_i)|}{\\max|y_i-h_t(x_i)|}\\right)\n",
    "$\n",
    "\n",
    "$\\eta$ is the learning rate and $L_i$ is a loss function constrained on $[0,1]$. $m$ is the number of instances of the training data. The initial weights $w_i^1$ are $\\frac{1}{m}$. The updated and normalized weights are then used to train a new model. This can be repeated several times. The final predictions are made by a weighted median of all models.\n",
    "\n",
    "Let us use Adaboost to improve our SVR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142e5ebb-1438-40db-9e27-dd6b12f99140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error of each validation in kJ/mol:\n",
      "[1.87352819 2.14775678 1.38423592 1.29054251 1.59111137]\n",
      "\n",
      "This is an average root mean square error of 1.66 kJ/mol with a standard deviation of 0.32 kJ/mol\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code from previous notebooks of this section\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "import numpy as np\n",
    "\n",
    "hb_data = pd.read_csv('HB_data.csv')\n",
    "train_set, test_set = train_test_split(hb_data, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = train_set['energy']\n",
    "X_train = train_set.drop(['energy'], axis=1)\n",
    "\n",
    "hb_data = pd.read_csv('HB_data.csv')\n",
    "train_set, test_set = train_test_split(hb_data, test_size=0.2, random_state=42)\n",
    "\n",
    "num_pipeline = make_pipeline(StandardScaler())\n",
    "cat_pipeline = make_pipeline(OneHotEncoder(sparse_output=False))\n",
    "\n",
    "preprocessing = ColumnTransformer([(\"num\",num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "                                        (\"cat\",cat_pipeline, make_column_selector(dtype_include=object)),])\n",
    "# end code from previous notebooks of this section\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model_adaboost = make_pipeline(preprocessing,AdaBoostRegressor(learning_rate=0.7,n_estimators=5,loss='linear',\n",
    "                             estimator=SVR(kernel=\"rbf\", C=5000, gamma='scale', epsilon=0.1),random_state=42))\n",
    "\n",
    "scores = -cross_val_score(model_adaboost, X_train, y_train, scoring='neg_root_mean_squared_error', cv=5)\n",
    "\n",
    "print(f\"Root mean square error of each validation in kJ/mol:\\n{scores}\\n\")\n",
    "print(\"This is an average root mean square error of %0.2f kJ/mol with a standard deviation of %0.2f kJ/mol\\n\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6112262-f1ee-4612-84c4-82fbff702b50",
   "metadata": {},
   "source": [
    "Thus, Adaboost resulted in an improved SVR model. We have run in total 5 boosting cycles. Adaboost can capture complex pattern by adapting to difficult cases. It reduces the risk of overfitting by adjusting sample weights. However, Adaboost is sensitive to outliers and noisy data. Furthermore, it might struggle with imbalanced data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980864f7-e702-4487-9005-936121536e13",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c7e973-1b1b-4409-b29e-eacd6c96b4cd",
   "metadata": {},
   "source": [
    "Gradient boosting fit a new predictor to the residual errors made by the previous predictor. It uses decision trees in Scikit-learn but is not limited to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949aba8d-8657-4bdb-ab90-27b1528ec816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error of each validation in kJ/mol:\n",
      "[1.42749115 1.51116803 1.21027719 1.11317383 1.16739953]\n",
      "\n",
      "This is an average root mean square error of 1.29 kJ/mol with a standard deviation of 0.16 kJ/mol\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model_gradient = make_pipeline(preprocessing,GradientBoostingRegressor(init=SVR(kernel=\"rbf\", gamma='scale', C=5000, epsilon=0.1), \n",
    "                            random_state=42, learning_rate=0.1, n_estimators=70, max_depth=5, min_samples_split=2))\n",
    "\n",
    "scores = -cross_val_score(model_gradient, X_train, y_train, scoring='neg_root_mean_squared_error', cv=5)\n",
    "\n",
    "print(f\"Root mean square error of each validation in kJ/mol:\\n{scores}\\n\")\n",
    "print(\"This is an average root mean square error of %0.2f kJ/mol with a standard deviation of %0.2f kJ/mol\\n\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af5fb40-b35d-4a05-9b48-ce8d318787ec",
   "metadata": {},
   "source": [
    "This is our best model. The learning rate scales the contribution of each tree. A low value results in more trees to fit the training set but the predictions generalize commonly better. The hyperparameter \"n_estimators\" defines the number of boosting stages. The minimum number of samples required to split a node is set by min_samples_split. The maxinum number of nodes  from the root node to the leaves for each tree is set by \"max__depth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dcab8e-5f55-4ca7-a8ad-6b23e0b669d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ai-school",
   "language": "python",
   "name": "venv-ai-school"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
