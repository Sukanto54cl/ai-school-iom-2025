{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41139d86-ff77-490e-baae-724c898ba27f",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea15bff5-e6c0-4429-89de-e00424642262",
   "metadata": {},
   "source": [
    "Before we train our first regression model, the data must be prepared for the machine learning approach. For example, we must transform the character values to numerical attributes as in the previous subsection. Furthermore, some algorithm are sensitive to the scaling of the input features, e.g. support vector regression or k-nearest neighbor regression. It might be also that some values in selected columns are missing. We will show some of these transformations first before we setup a pipeline which will make the data handling much easier. \n",
    "\n",
    "At first, we will pick all input data X from our training data by using drop from pandas to remove the energy column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2e6a71-b29a-48a5-9a3f-4f2abd268fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bo-acc</th>\n",
       "      <th>bo-donor</th>\n",
       "      <th>q-acc</th>\n",
       "      <th>q-donor</th>\n",
       "      <th>q-hatom</th>\n",
       "      <th>dist-dh</th>\n",
       "      <th>dist-ah</th>\n",
       "      <th>atomtype-acc</th>\n",
       "      <th>atomtype-don</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.2549</td>\n",
       "      <td>1.1085</td>\n",
       "      <td>0.167554</td>\n",
       "      <td>-0.178104</td>\n",
       "      <td>-0.030259</td>\n",
       "      <td>0.965293</td>\n",
       "      <td>2.034707</td>\n",
       "      <td>S</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>0.1725</td>\n",
       "      <td>0.8950</td>\n",
       "      <td>-0.261959</td>\n",
       "      <td>0.276786</td>\n",
       "      <td>0.108965</td>\n",
       "      <td>1.055615</td>\n",
       "      <td>1.844385</td>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>0.2110</td>\n",
       "      <td>1.0962</td>\n",
       "      <td>0.205667</td>\n",
       "      <td>-0.182710</td>\n",
       "      <td>-0.008530</td>\n",
       "      <td>0.970337</td>\n",
       "      <td>2.129663</td>\n",
       "      <td>S</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.1783</td>\n",
       "      <td>1.0630</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.211999</td>\n",
       "      <td>0.049246</td>\n",
       "      <td>0.996096</td>\n",
       "      <td>1.903904</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>0.0623</td>\n",
       "      <td>1.0837</td>\n",
       "      <td>-0.215193</td>\n",
       "      <td>-0.064738</td>\n",
       "      <td>0.078079</td>\n",
       "      <td>0.972071</td>\n",
       "      <td>2.327929</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bo-acc  bo-donor     q-acc   q-donor   q-hatom   dist-dh   dist-ah  \\\n",
       "63    0.2549    1.1085  0.167554 -0.178104 -0.030259  0.965293  2.034707   \n",
       "1316  0.1725    0.8950 -0.261959  0.276786  0.108965  1.055615  1.844385   \n",
       "1018  0.2110    1.0962  0.205667 -0.182710 -0.008530  0.970337  2.129663   \n",
       "1046  0.1783    1.0630 -0.377025 -0.211999  0.049246  0.996096  1.903904   \n",
       "1149  0.0623    1.0837 -0.215193 -0.064738  0.078079  0.972071  2.327929   \n",
       "\n",
       "     atomtype-acc atomtype-don  \n",
       "63              S            O  \n",
       "1316            O            N  \n",
       "1018            S            O  \n",
       "1046            O            O  \n",
       "1149            O            O  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code from previous notebooks of this section\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "hb_data = pd.read_csv('HB_data.csv')\n",
    "train_set, test_set = train_test_split(hb_data, test_size=0.2, random_state=42)\n",
    "# end code from previous notebooks of this section\n",
    "\n",
    "X_train = train_set.drop(['energy'], axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257219c8-3372-4cfa-beff-9b79bdb6e844",
   "metadata": {},
   "source": [
    "The first row refers to the instance in the original data set. A common problem might be that your data is not complete. Selected values might be missing in some columns. You can remove these instances with the dropna feature of pandas:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51cff2ce-c3bb-4d25-9ba1-b6b65473a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dropna(subset=[\"q-acc\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1d4dc-6a08-4edc-bff4-28ed495ed376",
   "metadata": {},
   "source": [
    "Since our data set is complete, no instance will be removed from our example.\n",
    "\n",
    "You can also provide a guess by fillna of pandas. Here is a code example where each missing value is replaced by the average value of this column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085ceb60-fd5c-4f59-b226-842d90b7b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = X_train[\"q-acc\"].median() \n",
    "X_train.fillna({\"q-acc\": median}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a697f-14b5-49a8-8517-66e6d58099e4",
   "metadata": {},
   "source": [
    "You have already seen in a previous section how to transform characters into numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea895b46-769c-4ae0-a6a6-bb11c4588341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five instances before transformation:\n",
      "     atomtype-acc\n",
      "63              S\n",
      "1316            O\n",
      "1018            S\n",
      "1046            O\n",
      "1149            O\n",
      "\n",
      "First five instances after transformation:\n",
      "[[4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "atomtype_acc_c = train_set[[\"atomtype-acc\"]]\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "atomtype_acc_num = ordinal_encoder.fit_transform(atomtype_acc_c)\n",
    "\n",
    "print(\"First five instances before transformation:\")\n",
    "print(atomtype_acc_c[:5])\n",
    "print(\"\\nFirst five instances after transformation:\")\n",
    "print(atomtype_acc_num[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6a6b7-bd18-4c91-bbe6-58c1a8a003bb",
   "metadata": {},
   "source": [
    "You can check the numbers assigned to each element by following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7d34b0-a395-4a73-b19e-9b68b564af3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Cl', 'F', 'N', 'O', 'S'], dtype=object)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b87d11-a0b2-4f7e-8968-94e4760ced1d",
   "metadata": {},
   "source": [
    "Please note, it starts counting with 0. Passing this to a regression algorithm is not reasonable since it will assume that two nearby values are more similar than two distant values. This is not the case! \n",
    "\n",
    "A common solution is to create one binary attribute per category: The attribute for the element C is equal to 1 when the element is \"C\" and 0 otherwise. Scikit-learn provides a OneHotEncoder class to convert categorical values into one-hot vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b21bae-959e-45fe-9648-eb714d5bc67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five instances before transformation:\n",
      "     atomtype-acc\n",
      "63              S\n",
      "1316            O\n",
      "1018            S\n",
      "1046            O\n",
      "1149            O\n",
      "\n",
      "First five instances after transformation:\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "atomtype_acc_c = train_set[[\"atomtype-acc\"]]\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "atomtype_acc_1hot = cat_encoder.fit_transform(atomtype_acc_c)\n",
    "\n",
    "print(\"First five instances before transformation:\")\n",
    "print(atomtype_acc_c[:5])\n",
    "print(\"\\nFirst five instances after transformation:\")\n",
    "print(atomtype_acc_1hot[:5].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e3665-ad24-416f-a064-1517e24bd181",
   "metadata": {},
   "source": [
    "You can get the categories similar as for the ordinal encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37fc5fe4-936b-499d-a893-d421f76fc4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Cl', 'F', 'N', 'O', 'S'], dtype=object)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb7249-229d-4472-864c-28c49d0d556d",
   "metadata": {},
   "source": [
    "Please note, the one-hot encoder will raise an exception when the trained model will be applied to an instance with an unknown category. \n",
    "\n",
    "Most machine learning approaches do not perform well on input features of different scales. Thus, it is important that you apply a feature scaling before passing the input features to the regression model. Scikit-Learn provides two approaches. \n",
    "\n",
    "In min-max scaling (also called normalization), the values are scaled in a given range (default is between 0 and 1). This is performed by subtracting the minimum value and dividing it by the difference between the minimum and the maximum. Please note, some models work best on selected scales. For example, neural networks work best with a zero-mean input. Thus, a range from -1 to 1 is desirable since the activation function of neural networks change strongest close to zero. A code example is given below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29ec3596-538a-4a25-96f1-80ecf1e528ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five instances before transformation:\n",
      "      bo-acc  bo-donor     q-acc   q-donor   q-hatom   dist-dh   dist-ah\n",
      "63    0.2549    1.1085  0.167554 -0.178104 -0.030259  0.965293  2.034707\n",
      "1316  0.1725    0.8950 -0.261959  0.276786  0.108965  1.055615  1.844385\n",
      "1018  0.2110    1.0962  0.205667 -0.182710 -0.008530  0.970337  2.129663\n",
      "1046  0.1783    1.0630 -0.377025 -0.211999  0.049246  0.996096  1.903904\n",
      "1149  0.0623    1.0837 -0.215193 -0.064738  0.078079  0.972071  2.327929\n",
      "\n",
      "First five instances after transformation:\n",
      "[[-0.0056319   0.4504764   0.72635861 -0.73536865 -0.38518884 -0.86668117\n",
      "  -0.27765376]\n",
      " [-0.37688669 -0.49567915  0.03100271  0.74724875  0.68813457 -0.52531492\n",
      "  -0.56979647]\n",
      " [-0.20342419  0.39596721  0.78806129 -0.75038093 -0.21767286 -0.84761657\n",
      "  -0.1318979 ]\n",
      " [-0.35075467  0.24883669 -0.15528228 -0.84584221  0.22774124 -0.75026229\n",
      "  -0.47843542]\n",
      " [-0.87339491  0.34057168  0.10671407 -0.36587624  0.45002428 -0.84106274\n",
      "   0.17243787]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train_num = train_set.drop(['energy','atomtype-acc','atomtype-don'], axis=1)\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_num_min_max = min_max_scaler.fit_transform(X_train_num)\n",
    "\n",
    "print(\"First five instances before transformation:\")\n",
    "print(X_train_num[:5])\n",
    "print(\"\\nFirst five instances after transformation:\")\n",
    "print(X_train_num_min_max[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f7522-892d-41f5-9867-a5cb5511732c",
   "metadata": {},
   "source": [
    "Standardization is an alternative approach. First, it subtracts the mean value. Thus, the mean value is shifted to zero. Subsequently, the results are divided by the standard deviation. Thus, standardization is not restricted to values within a specific range and much less affected by outliers. You can setup this with following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4af1ef0-480e-4690-b81c-0369476b696d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five instances before transformation:\n",
      "      bo-acc  bo-donor     q-acc   q-donor   q-hatom   dist-dh   dist-ah\n",
      "63    0.2549    1.1085  0.167554 -0.178104 -0.030259  0.965293  2.034707\n",
      "1316  0.1725    0.8950 -0.261959  0.276786  0.108965  1.055615  1.844385\n",
      "1018  0.2110    1.0962  0.205667 -0.182710 -0.008530  0.970337  2.129663\n",
      "1046  0.1783    1.0630 -0.377025 -0.211999  0.049246  0.996096  1.903904\n",
      "1149  0.0623    1.0837 -0.215193 -0.064738  0.078079  0.972071  2.327929\n",
      "\n",
      "First five instances after transformation:\n",
      "[[ 1.5289407   1.08558958  1.30277591 -1.04442968 -1.7550715  -0.69528915\n",
      "  -0.40588825]\n",
      " [ 0.4406232  -1.34253292 -0.43263547  1.96015514  1.51792787  0.11637256\n",
      "  -1.0615082 ]\n",
      " [ 0.94912106  0.94570243  1.4567683  -1.07485268 -1.24424719 -0.64995954\n",
      "  -0.07878632]\n",
      " [ 0.51722807  0.56812085 -0.89755003 -1.26830886  0.11400147 -0.41848162\n",
      "  -0.85647783]\n",
      " [-1.01486938  0.80354069 -0.24368135 -0.29563819  0.79183281 -0.63437658\n",
      "   0.60419701]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_num = train_set.drop(['energy','atomtype-acc','atomtype-don'], axis=1)\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "X_train_num_std = std_scaler.fit_transform(X_train_num)\n",
    "\n",
    "print(\"First five instances before transformation:\")\n",
    "print(X_train_num[:5])\n",
    "print(\"\\nFirst five instances after transformation:\")\n",
    "print(X_train_num_std[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf00205f-f943-487e-849d-779b9b7f4b5b",
   "metadata": {},
   "source": [
    "Please note, both approaches are not suited for data with a heavy tail. Most data will be still in a small range after the transformation. Thus, you should shrink the heavy tail before scaling! \n",
    "\n",
    "Scikit-learn provides the possibility to setup a pipeline for the transformations. This will make it much easier to execute everything correctly, for example to run the fitted model on the test data set. Below is the example for a standard scaling and a one-hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58ddd63f-f4aa-470e-befa-6496e051f3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the first 5 instances of our input features after the pipeline:\n",
      "[[ 1.5289407   1.08558958  1.30277591 -1.04442968 -1.7550715  -0.69528915\n",
      "  -0.40588825  0.          0.          0.          0.          1.\n",
      "   0.          0.          1.          0.        ]\n",
      " [ 0.4406232  -1.34253292 -0.43263547  1.96015514  1.51792787  0.11637256\n",
      "  -1.0615082   0.          0.          0.          1.          0.\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 0.94912106  0.94570243  1.4567683  -1.07485268 -1.24424719 -0.64995954\n",
      "  -0.07878632  0.          0.          0.          0.          1.\n",
      "   0.          0.          1.          0.        ]\n",
      " [ 0.51722807  0.56812085 -0.89755003 -1.26830886  0.11400147 -0.41848162\n",
      "  -0.85647783  0.          0.          0.          1.          0.\n",
      "   0.          0.          1.          0.        ]\n",
      " [-1.01486938  0.80354069 -0.24368135 -0.29563819  0.79183281 -0.63437658\n",
      "   0.60419701  0.          0.          0.          1.          0.\n",
      "   0.          0.          1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "import numpy as np\n",
    "\n",
    "#This line will get solely the column energy from the training data set.\n",
    "y_train = train_set['energy']\n",
    "#This line will remove solely the column energy from the training data set.\n",
    "X_train = train_set.drop(['energy'], axis=1)\n",
    "\n",
    "num_pipeline = make_pipeline(StandardScaler())\n",
    "# If you want to give a specific name your pipeline, you can also use following two lines of code:\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# num_pipeline = Pipeline([(\"scaling\", StandardScaler()),])\n",
    "cat_pipeline = make_pipeline(OneHotEncoder()) \n",
    "\n",
    "# The num_pipeline is solely applied to numerical values while cat_pipelines is used on characters\n",
    "preprocessing = ColumnTransformer([(\"num\",num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "                                        (\"cat\",cat_pipeline, make_column_selector(dtype_include=object))])\n",
    "# You can also apply a transformer solely to selected features:\n",
    "#cat_attribs = [\"atomtype-acc\"]\n",
    "#preprocessing = ColumnTransformer([(\"cat-acc\",cat_pipeline, cat_attribs)],remainder='passthrough')\n",
    "\n",
    "X_train_prepared = preprocessing.fit_transform(X_train)\n",
    "\n",
    "print(\"These are the first 5 instances of our input features after the pipeline:\")\n",
    "print(X_train_prepared[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08c7bf-7b5e-4d82-809c-d1e24c52bd58",
   "metadata": {},
   "source": [
    "Thus, our data set is now ready for training our first regression model. Please note, never use fit() or fit_transform() on anything else then the training data! You can use transform() on any data. You can get the column names of your pipeline by using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fef725b-ddbc-42f5-92a0-3302e433d4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__bo-acc</th>\n",
       "      <th>num__bo-donor</th>\n",
       "      <th>num__q-acc</th>\n",
       "      <th>num__q-donor</th>\n",
       "      <th>num__q-hatom</th>\n",
       "      <th>num__dist-dh</th>\n",
       "      <th>num__dist-ah</th>\n",
       "      <th>cat__atomtype-acc_Cl</th>\n",
       "      <th>cat__atomtype-acc_F</th>\n",
       "      <th>cat__atomtype-acc_N</th>\n",
       "      <th>cat__atomtype-acc_O</th>\n",
       "      <th>cat__atomtype-acc_S</th>\n",
       "      <th>cat__atomtype-don_F</th>\n",
       "      <th>cat__atomtype-don_N</th>\n",
       "      <th>cat__atomtype-don_O</th>\n",
       "      <th>cat__atomtype-don_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.528941</td>\n",
       "      <td>1.085590</td>\n",
       "      <td>1.302776</td>\n",
       "      <td>-1.044430</td>\n",
       "      <td>-1.755071</td>\n",
       "      <td>-0.695289</td>\n",
       "      <td>-0.405888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.440623</td>\n",
       "      <td>-1.342533</td>\n",
       "      <td>-0.432635</td>\n",
       "      <td>1.960155</td>\n",
       "      <td>1.517928</td>\n",
       "      <td>0.116373</td>\n",
       "      <td>-1.061508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.949121</td>\n",
       "      <td>0.945702</td>\n",
       "      <td>1.456768</td>\n",
       "      <td>-1.074853</td>\n",
       "      <td>-1.244247</td>\n",
       "      <td>-0.649960</td>\n",
       "      <td>-0.078786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.517228</td>\n",
       "      <td>0.568121</td>\n",
       "      <td>-0.897550</td>\n",
       "      <td>-1.268309</td>\n",
       "      <td>0.114001</td>\n",
       "      <td>-0.418482</td>\n",
       "      <td>-0.856478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.014869</td>\n",
       "      <td>0.803541</td>\n",
       "      <td>-0.243681</td>\n",
       "      <td>-0.295638</td>\n",
       "      <td>0.791833</td>\n",
       "      <td>-0.634377</td>\n",
       "      <td>0.604197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>0.699495</td>\n",
       "      <td>-1.220842</td>\n",
       "      <td>1.126638</td>\n",
       "      <td>1.097076</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>-0.107656</td>\n",
       "      <td>1.091242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>0.900252</td>\n",
       "      <td>-0.223436</td>\n",
       "      <td>-2.857524</td>\n",
       "      <td>-1.191333</td>\n",
       "      <td>-0.740334</td>\n",
       "      <td>-0.023279</td>\n",
       "      <td>0.724262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>-0.777130</td>\n",
       "      <td>0.341799</td>\n",
       "      <td>0.155201</td>\n",
       "      <td>-0.975407</td>\n",
       "      <td>-0.225631</td>\n",
       "      <td>-0.235411</td>\n",
       "      <td>0.106780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>0.020617</td>\n",
       "      <td>0.327015</td>\n",
       "      <td>-0.032606</td>\n",
       "      <td>0.012793</td>\n",
       "      <td>-1.577556</td>\n",
       "      <td>2.758347</td>\n",
       "      <td>-1.385322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>2.184044</td>\n",
       "      <td>-1.547246</td>\n",
       "      <td>1.382905</td>\n",
       "      <td>1.018680</td>\n",
       "      <td>-0.542061</td>\n",
       "      <td>-0.026322</td>\n",
       "      <td>-0.317851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1310 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num__bo-acc  num__bo-donor  num__q-acc  num__q-donor  num__q-hatom  \\\n",
       "0        1.528941       1.085590    1.302776     -1.044430     -1.755071   \n",
       "1        0.440623      -1.342533   -0.432635      1.960155      1.517928   \n",
       "2        0.949121       0.945702    1.456768     -1.074853     -1.244247   \n",
       "3        0.517228       0.568121   -0.897550     -1.268309      0.114001   \n",
       "4       -1.014869       0.803541   -0.243681     -0.295638      0.791833   \n",
       "...           ...            ...         ...           ...           ...   \n",
       "1305     0.699495      -1.220842    1.126638      1.097076      0.425000   \n",
       "1306     0.900252      -0.223436   -2.857524     -1.191333     -0.740334   \n",
       "1307    -0.777130       0.341799    0.155201     -0.975407     -0.225631   \n",
       "1308     0.020617       0.327015   -0.032606      0.012793     -1.577556   \n",
       "1309     2.184044      -1.547246    1.382905      1.018680     -0.542061   \n",
       "\n",
       "      num__dist-dh  num__dist-ah  cat__atomtype-acc_Cl  cat__atomtype-acc_F  \\\n",
       "0        -0.695289     -0.405888                   0.0                  0.0   \n",
       "1         0.116373     -1.061508                   0.0                  0.0   \n",
       "2        -0.649960     -0.078786                   0.0                  0.0   \n",
       "3        -0.418482     -0.856478                   0.0                  0.0   \n",
       "4        -0.634377      0.604197                   0.0                  0.0   \n",
       "...            ...           ...                   ...                  ...   \n",
       "1305     -0.107656      1.091242                   0.0                  0.0   \n",
       "1306     -0.023279      0.724262                   0.0                  1.0   \n",
       "1307     -0.235411      0.106780                   0.0                  0.0   \n",
       "1308      2.758347     -1.385322                   0.0                  0.0   \n",
       "1309     -0.026322     -0.317851                   0.0                  0.0   \n",
       "\n",
       "      cat__atomtype-acc_N  cat__atomtype-acc_O  cat__atomtype-acc_S  \\\n",
       "0                     0.0                  0.0                  1.0   \n",
       "1                     0.0                  1.0                  0.0   \n",
       "2                     0.0                  0.0                  1.0   \n",
       "3                     0.0                  1.0                  0.0   \n",
       "4                     0.0                  1.0                  0.0   \n",
       "...                   ...                  ...                  ...   \n",
       "1305                  0.0                  0.0                  1.0   \n",
       "1306                  0.0                  0.0                  0.0   \n",
       "1307                  0.0                  1.0                  0.0   \n",
       "1308                  0.0                  1.0                  0.0   \n",
       "1309                  0.0                  0.0                  1.0   \n",
       "\n",
       "      cat__atomtype-don_F  cat__atomtype-don_N  cat__atomtype-don_O  \\\n",
       "0                     0.0                  0.0                  1.0   \n",
       "1                     0.0                  1.0                  0.0   \n",
       "2                     0.0                  0.0                  1.0   \n",
       "3                     0.0                  0.0                  1.0   \n",
       "4                     0.0                  0.0                  1.0   \n",
       "...                   ...                  ...                  ...   \n",
       "1305                  0.0                  1.0                  0.0   \n",
       "1306                  0.0                  1.0                  0.0   \n",
       "1307                  0.0                  1.0                  0.0   \n",
       "1308                  0.0                  0.0                  0.0   \n",
       "1309                  0.0                  1.0                  0.0   \n",
       "\n",
       "      cat__atomtype-don_S  \n",
       "0                     0.0  \n",
       "1                     0.0  \n",
       "2                     0.0  \n",
       "3                     0.0  \n",
       "4                     0.0  \n",
       "...                   ...  \n",
       "1305                  0.0  \n",
       "1306                  0.0  \n",
       "1307                  0.0  \n",
       "1308                  1.0  \n",
       "1309                  0.0  \n",
       "\n",
       "[1310 rows x 16 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(X_train_prepared, columns=preprocessing.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155aa96-6b6d-4710-8874-4b0fd58645a9",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b14007-1597-4e13-9db5-7dbbc2d7f7da",
   "metadata": {},
   "source": [
    "Make a pipeline with a MinMaxScaler in the range from 0 to 1 for the input features of \"housing_data.csv\" which is in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0f58f5-30ad-4eec-8c25-62192d3d68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff4ab6-2e52-4b46-ab34-1f541311bcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ai-school",
   "language": "python",
   "name": "venv-ai-school"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
